"""
–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ HTTP (–±–µ–∑ Selenium)
"""
import logging
from typing import Optional, Tuple
from bs4 import BeautifulSoup
from backend.config import settings

logger = logging.getLogger("competitor_monitor.http_parser")

class HTTPParserService:
    """–ë—ã—Å—Ç—Ä—ã–π HTTP –ø–∞—Ä—Å–∏–Ω–≥ –±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–∞"""
    
    def __init__(self):
        logger.info("=" * 50)
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP Parser —Å–µ—Ä–≤–∏—Å–∞")
        logger.info("HTTP Parser —Å–µ—Ä–≤–∏—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚úì")
        logger.info("=" * 50)
    
    async def parse_url(self, url: str) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:
        """
        –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ URL —á–µ—Ä–µ–∑ HTTP
        
        Returns:
            title, h1, first_paragraph, error
        """
        logger.info("=" * 50)
        logger.info(f"üåê HTTP –ü–ê–†–°–ò–ù–ì: {url}")
        
        try:
            import httpx
            import asyncio
            
            logger.info("  üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
            
            # –ë—ã—Å—Ç—Ä—ã–π HTTP –∑–∞–ø—Ä–æ—Å
            response = await asyncio.wait_for(
                httpx.AsyncClient().aget(url, timeout=15.0, follow_redirects=True),
                timeout=15.0
            )
            
            logger.info(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤, —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            
            if response.status_code != 200:
                logger.warning(f"  ‚ö†Ô∏è –ù–µ–æ–±—ã—á–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Title
            title = soup.title.string.strip() if soup.title and soup.title.string else None
            logger.info(f"  üìå Title: {title[:60] if title else 'N/A'}...")
            
            # H1
            h1 = None
            h1_tag = soup.find('h1')
            if h1_tag:
                h1 = h1_tag.get_text(strip=True)
                if len(h1) > 500:
                    h1 = h1[:500]
            logger.info(f"  üìå H1: {h1[:60] if h1 else 'N/A'}...")
            
            # –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü
            first_paragraph = None
            p_tags = soup.find_all('p')
            for p in p_tags:
                text = p.get_text(strip=True)
                if len(text) > 50:
                    first_paragraph = text[:500]
                    logger.info(f"  üìå –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü: {first_paragraph[:60]}...")
                    break
            
            logger.info("  ‚úÖ HTTP –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω")
            logger.info("=" * 50)
            
            return title, h1, first_paragraph, None
            
        except asyncio.TimeoutError:
            logger.error("  ‚úó –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏")
            logger.error("=" * 50)
            return None, None, None, "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
            
        except Exception as e:
            logger.error(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
            logger.error("=" * 50)
            return None, None, None, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)[:100]}"


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
http_parser_service = HTTPParserService()